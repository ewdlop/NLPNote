"""
åŒå€«é¡å‹è«–NLPæ¸¬è©¦èˆ‡æ¼”ç¤º (Homotopy Type Theory NLP Tests and Examples)

This module provides comprehensive tests and examples for the HomotopyTypeTheoryNLP implementation,
demonstrating the integration of Homotopy Type Theory concepts with natural language processing
and the existing PathIntegralNLP framework.
"""

import sys
import os

# Add the current directory to the path to import our modules
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from HomotopyTypeTheoryNLP import (
        HomotopyTypeTheoryNLP, HoTTPathType, SemanticType, 
        SemanticPath, HomotopyEquivalence, HigherInductiveType
    )
    from PathIntegralNLP import TianDaoPath
    HOTT_AVAILABLE = True
except ImportError as e:
    print(f"Import error: {e}")
    HOTT_AVAILABLE = False


def test_basic_hott_concepts():
    """æ¸¬è©¦åŸºæœ¬HoTTæ¦‚å¿µ"""
    print("=" * 60)
    print("åŸºæœ¬åŒå€«é¡å‹è«–æ¦‚å¿µæ¸¬è©¦ (Basic HoTT Concepts Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    print("\n1. èªç¾©é¡å‹è¨»å†Šæ¸¬è©¦")
    print("-" * 30)
    
    # è¨»å†Šèªç¾©é¡å‹
    emotions_type = hott_nlp.register_semantic_type(
        "emotions", 
        ["å¿«æ¨‚", "æ‚²å‚·", "æ†¤æ€’", "å¹³éœ"],
        dimension=2,
        properties={"category": "emotions", "valence": "mixed"}
    )
    
    actions_type = hott_nlp.register_semantic_type(
        "actions",
        ["è·‘æ­¥", "æ€è€ƒ", "èªªè©±", "ä¼‘æ¯"],
        dimension=2,
        properties={"category": "actions", "energy": "variable"}
    )
    
    print(f"æƒ…æ„Ÿé¡å‹: {emotions_type.name}, å…ƒç´ æ•¸: {len(emotions_type.elements)}")
    print(f"è¡Œå‹•é¡å‹: {actions_type.name}, å…ƒç´ æ•¸: {len(actions_type.elements)}")
    
    print("\n2. èªç¾©è·¯å¾‘æ§‹é€ æ¸¬è©¦")
    print("-" * 30)
    
    # æ§‹é€ è·¯å¾‘
    path1 = hott_nlp.construct_semantic_path("å¿«æ¨‚", "è·‘æ­¥", HoTTPathType.EQUIVALENCE)
    path2 = hott_nlp.construct_semantic_path("è·‘æ­¥", "å¥åº·", HoTTPathType.TRANSPORT)
    
    print(f"è·¯å¾‘1: {path1.source} â†’ {path1.target} (é¡å‹: {path1.path_type.value})")
    print(f"è·¯å¾‘2: {path2.source} â†’ {path2.target} (é¡å‹: {path2.path_type.value})")
    
    # æ¸¬è©¦è·¯å¾‘åˆæˆ
    composed = hott_nlp.path_space_calculator.compose_paths(path1, path2)
    if composed:
        print(f"åˆæˆè·¯å¾‘: {composed.source} â†’ {composed.target}")
        print(f"åˆæˆè­‰æ˜é …: {composed.proof_term}")
    else:
        print("è·¯å¾‘ç„¡æ³•åˆæˆ")
    
    print("\n3. ä¸€å…ƒæ€§ç­‰åƒ¹æ¸¬è©¦")
    print("-" * 30)
    
    # æ¸¬è©¦ç­‰åƒ¹æ€§
    equivalence = hott_nlp.univalence_calculator.construct_equivalence(emotions_type, actions_type)
    if equivalence:
        print(f"ç™¼ç¾ç­‰åƒ¹: {equivalence.type_a.name} â‰ƒ {equivalence.type_b.name}")
        print(f"ç­‰åƒ¹è­‰æ˜: {equivalence.equivalence_proof}")
    else:
        print("æœªç™¼ç¾ç­‰åƒ¹é—œä¿‚")
    
    print("\n4. æ†ç­‰è·¯å¾‘æ¸¬è©¦")
    print("-" * 30)
    
    # æ†ç­‰è·¯å¾‘
    id_path = hott_nlp.path_space_calculator.identity_path("æ¦‚å¿µ")
    print(f"æ†ç­‰è·¯å¾‘: {id_path.source} = {id_path.target}")
    print(f"è­‰æ˜é …: {id_path.proof_term}")
    
    return True


def test_path_space_analysis():
    """æ¸¬è©¦è·¯å¾‘ç©ºé–“åˆ†æ"""
    print("\n" + "=" * 60)
    print("è·¯å¾‘ç©ºé–“åˆ†ææ¸¬è©¦ (Path Space Analysis Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "æ°´æµå‘å±±ï¼Œå±±æŒ‡å‘å¤©ï¼Œå¤©è¦†è“‹åœ°",
        "æ€è€ƒç”¢ç”Ÿæƒ³æ³•ï¼Œæƒ³æ³•è®Šæˆè¨ˆåŠƒï¼Œè¨ˆåŠƒå°è‡´è¡Œå‹•",
        "From concept to reality through careful planning",
        "å­¸ç¿’å¸¶ä¾†çŸ¥è­˜ï¼ŒçŸ¥è­˜ä¿ƒé€²ç†è§£ï¼Œç†è§£å‰µé€ æ™ºæ…§",
        "èŠ±é–‹èŠ±è½ï¼Œæ½®èµ·æ½®è½ï¼Œäººç”Ÿå¦‚å¤¢"
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\næ¸¬è©¦ {i}: '{text}'")
        print("-" * 40)
        
        analysis = hott_nlp.construct_path_space_analysis(text)
        
        print(f"æ¦‚å¿µæå–: {analysis['concepts']}")
        print(f"è·¯å¾‘åˆ†æ: {analysis['path_analysis']}")
        print(f"æ§‹é€ è·¯å¾‘: {len(analysis.get('paths', []))}")
        print(f"åˆæˆè·¯å¾‘: {len(analysis.get('composed_paths', []))}")
        print(f"ç­‰åƒ¹é—œä¿‚: {len(analysis.get('equivalences', []))}")
        
        hott_metrics = analysis.get('hott_analysis', {})
        print(f"HoTT æŒ‡æ¨™:")
        print(f"  - è·¯å¾‘æ§‹é€ æ•¸: {hott_metrics.get('paths_constructed', 0)}")
        print(f"  - åŒå€«æ§‹é€ æ•¸: {hott_metrics.get('homotopies_constructed', 0)}")
        print(f"  - ä¸€å…ƒæ€§æ‡‰ç”¨: {hott_metrics.get('univalence_applications', 0)}")
        
        # å¤©é“æ•´åˆçµæœ
        tian_dao = analysis.get('tian_dao_integration', {})
        if tian_dao.get('integration_successful', False):
            print(f"å¤©é“æ•´åˆæˆåŠŸ:")
            print(f"  - å¤©é“å°é½Šåº¦: {tian_dao.get('tian_dao_alignment', 0):.3f}")
            print(f"  - è‡ªç„¶æµå‹•: {tian_dao.get('natural_flow_score', 0):.3f}")
            print(f"  - å’Œè«§æŒ‡æ•¸: {tian_dao.get('harmony_index', 0):.3f}")
        else:
            print(f"å¤©é“æ•´åˆ: {tian_dao.get('integration_error', 'æœªé›†æˆ')}")
    
    return True


def test_univalence_applications():
    """æ¸¬è©¦ä¸€å…ƒæ€§åŸç†æ‡‰ç”¨"""
    print("\n" + "=" * 60)
    print("ä¸€å…ƒæ€§åŸç†æ‡‰ç”¨æ¸¬è©¦ (Univalence Principle Applications Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    # æ¸¬è©¦èªç¾©ç­‰åƒ¹åˆ†æ
    text_pairs = [
        ("å¤§è±¡å¾ˆå¤§", "å·¨å¤§çš„å‹•ç‰©"),
        ("å¿«é€Ÿå¥”è·‘", "è¿…é€Ÿç§»å‹•"),
        ("æ€è€ƒå•é¡Œ", "contemplating issues"),
        ("ç¾éº—çš„èŠ±æœµ", "æ¼‚äº®çš„æ¤ç‰©"),
        ("å›°é›£çš„æŒ‘æˆ°", "è‰±é›£çš„ä»»å‹™")
    ]
    
    for i, (text1, text2) in enumerate(text_pairs, 1):
        print(f"\nä¸€å…ƒæ€§åˆ†æ {i}: '{text1}' vs '{text2}'")
        print("-" * 50)
        
        analysis = hott_nlp.univalence_based_semantic_analysis(text1, text2)
        
        print(f"æ–‡æœ¬1æ¦‚å¿µ: {analysis['text1_concepts']}")
        print(f"æ–‡æœ¬2æ¦‚å¿µ: {analysis['text2_concepts']}")
        print(f"ä¸€å…ƒæ€§ç­‰åƒ¹: {'æ˜¯' if analysis['univalent_equivalence'] else 'å¦'}")
        
        if analysis['equivalence_details']:
            details = analysis['equivalence_details']
            print(f"ç­‰åƒ¹è©³æƒ…:")
            print(f"  - å‰å‘æ˜ å°„: {details['forward_map']}")
            print(f"  - å¾Œå‘æ˜ å°„: {details['backward_map']}")
            print(f"  - ç­‰åƒ¹è­‰æ˜: {details['equivalence_proof']}")
        
        transport = analysis.get('semantic_transport', {})
        if transport:
            print(f"èªç¾©å‚³è¼¸ç¤ºä¾‹:")
            for orig, transported in list(transport.items())[:2]:
                print(f"  - {orig} â†’ {transported}")
        
        identity_types = analysis.get('identity_types_analysis', {})
        if identity_types:
            print(f"æ†ç­‰é¡å‹åˆ†æ: {len(identity_types)} å€‹æ†ç­‰è·¯å¾‘")
    
    return True


def test_higher_inductive_types():
    """æ¸¬è©¦é«˜éšæ­¸ç´é¡å‹"""
    print("\n" + "=" * 60)
    print("é«˜éšæ­¸ç´é¡å‹æ¸¬è©¦ (Higher Inductive Types Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    # æ¸¬è©¦æ–‡æœ¬çµæ§‹
    complex_texts = [
        "æ˜¥å¤©ä¾†äº†ï¼ŒèŠ±é–‹äº†ï¼Œé³¥å…’æ­Œå”±ï¼Œç”Ÿå‘½å¾©ç”¦ï¼Œå¤§åœ°å›æ˜¥",
        "å­¸ç¿’ç·¨ç¨‹ï¼Œç·´ç¿’ç®—æ³•ï¼Œåšé …ç›®ï¼Œæ‰¾å·¥ä½œï¼Œæˆç‚ºå·¥ç¨‹å¸«",
        "æ€è€ƒå•é¡Œï¼Œåˆ†æåŸå› ï¼Œå°‹æ‰¾æ–¹æ¡ˆï¼Œå¯¦æ–½è¨ˆåŠƒï¼Œè©•ä¼°çµæœ",
        "æ—¥å‡ºæ±æ–¹ï¼Œå…‰ç…§å¤§åœ°ï¼Œè¬ç‰©ç”Ÿé•·ï¼Œæ—¥è½è¥¿å±±ï¼Œå¤œæ™šé™è‡¨"
    ]
    
    for i, text in enumerate(complex_texts, 1):
        print(f"\né«˜éšæ­¸ç´é¡å‹ {i}: '{text}'")
        print("-" * 40)
        
        # æ§‹é€ é«˜éšæ­¸ç´é¡å‹
        hit = hott_nlp.construct_higher_inductive_type(f"text_structure_{i}", text)
        
        print(f"HIT åç¨±: {hit.name}")
        print(f"åŸºæœ¬æ§‹é€ å­æ•¸é‡: {len(hit.constructors)}")
        print(f"è·¯å¾‘æ§‹é€ å­æ•¸é‡: {len(hit.path_constructors)}")
        print(f"é€£è²«æ€§æ³•å‰‡: {len(hit.coherence_laws)}")
        
        print(f"æ§‹é€ å­ (å‰5å€‹): {hit.constructors[:5]}")
        
        if hit.path_constructors:
            print(f"è·¯å¾‘æ§‹é€ å­ç¤ºä¾‹:")
            for pc in hit.path_constructors[:3]:
                print(f"  - {pc.source} â†’ {pc.target} ({pc.path_type.value})")
        
        print(f"æ¶ˆé™¤è¦å‰‡: {list(hit.elimination_rules.keys())}")
    
    return True


def test_homotopy_analysis():
    """æ¸¬è©¦åŒå€«åˆ†æ"""
    print("\n" + "=" * 60)
    print("åŒå€«åˆ†ææ¸¬è©¦ (Homotopy Analysis Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    # æ¸¬è©¦é«˜éšè·¯å¾‘æ§‹é€ 
    print("\n1. é«˜éšè·¯å¾‘æ§‹é€ ")
    print("-" * 25)
    
    # å‰µå»ºå…©æ¢ç›¸åŒèµ·çµ‚é»çš„è·¯å¾‘
    path1 = hott_nlp.construct_semantic_path("é–‹å§‹", "çµæŸ", HoTTPathType.EQUIVALENCE)
    path2 = hott_nlp.construct_semantic_path("é–‹å§‹", "çµæŸ", HoTTPathType.TRANSPORT)
    
    print(f"è·¯å¾‘1: {path1.proof_term}")
    print(f"è·¯å¾‘2: {path2.proof_term}")
    
    # æ§‹é€ åŒå€«
    homotopy = hott_nlp.higher_path_calculator.construct_homotopy(path1, path2)
    if homotopy:
        print(f"åŒå€«è·¯å¾‘: {homotopy.proof_term}")
        print(f"åŒå€«å±¤æ¬¡: {homotopy.homotopy_level}")
        print(f"é€£è²«æ€§æ¢ä»¶: {len(homotopy.coherence_conditions)}")
    else:
        print("ç„¡æ³•æ§‹é€ åŒå€«")
    
    print("\n2. èªç¾©å‚³è¼¸æ¸¬è©¦")
    print("-" * 25)
    
    # æ¸¬è©¦èªç¾©å‚³è¼¸
    element = "æ¦‚å¿µA"
    transported = hott_nlp.higher_path_calculator.transport_along_path(path1, element)
    print(f"åŸå§‹å…ƒç´ : {element}")
    print(f"å‚³è¼¸å¾Œ: {transported}")
    
    print("\n3. é€†è·¯å¾‘æ§‹é€ ")
    print("-" * 25)
    
    # æ§‹é€ é€†è·¯å¾‘
    inverse = hott_nlp.path_space_calculator.inverse_path(path1)
    print(f"åŸè·¯å¾‘: {path1.proof_term}")
    print(f"é€†è·¯å¾‘: {inverse.proof_term}")
    print(f"é€†è·¯å¾‘æ–¹å‘: {inverse.source} â†’ {inverse.target}")
    
    return True


def test_comprehensive_analysis():
    """æ¸¬è©¦ç¶œåˆåˆ†æåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ç¶œåˆåŒå€«é¡å‹è«–åˆ†ææ¸¬è©¦ (Comprehensive HoTT Analysis Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    # è¤‡é›œæ¸¬è©¦æ–‡æœ¬
    complex_text = "åœ¨åŒå€«é¡å‹è«–ä¸­ï¼Œé¡å‹è¢«è¦–ç‚ºç©ºé–“ï¼Œå…ƒç´ è¢«è¦–ç‚ºé»ï¼Œç­‰å¼è¢«è¦–ç‚ºè·¯å¾‘ã€‚é€šéä¸€å…ƒæ€§åŸç†ï¼Œæˆ‘å€‘å¯ä»¥å°‡ç­‰åƒ¹æ€§å’ŒåŒä¸€æ€§çµ±ä¸€èµ·ä¾†ï¼Œé€™ç‚ºæ•¸å­¸åŸºç¤æä¾›äº†æ–°çš„è¦–è§’ã€‚"
    
    print(f"åˆ†ææ–‡æœ¬: '{complex_text}'")
    print("-" * 50)
    
    # åŸ·è¡Œç¶œåˆåˆ†æ
    comprehensive = hott_nlp.comprehensive_hott_analysis(complex_text)
    
    print("\nåŸºæœ¬è·¯å¾‘åˆ†æ:")
    basic = comprehensive['basic_path_analysis']
    print(f"  æ¦‚å¿µæ•¸é‡: {len(basic['concepts'])}")
    print(f"  æ§‹é€ è·¯å¾‘: {len(basic.get('paths', []))}")
    print(f"  ç­‰åƒ¹é—œä¿‚: {len(basic.get('equivalences', []))}")
    
    print("\né«˜éšæ­¸ç´é¡å‹:")
    hit_info = comprehensive['higher_inductive_type']
    print(f"  åç¨±: {hit_info['name']}")
    print(f"  æ§‹é€ å­æ•¸é‡: {hit_info['constructors_count']}")
    print(f"  è·¯å¾‘æ§‹é€ å­æ•¸é‡: {hit_info['path_constructors_count']}")
    
    print("\nåŒå€«ç¾¤åˆ†æ:")
    homotopy_groups = comprehensive['homotopy_groups']
    print(f"  Ï€â‚€ (é€£é€šåˆ†é‡): {homotopy_groups['Ï€_0']}")
    print(f"  Ï€â‚ (åŸºæœ¬ç¾¤): {homotopy_groups['Ï€_1']}")
    print(f"  æ¦‚å¿µå¾ªç’°: {homotopy_groups['concept_cycles']}")
    
    higher_groups = homotopy_groups.get('higher_groups', {})
    if higher_groups:
        print(f"  é«˜éšåŒå€«ç¾¤: {list(higher_groups.keys())}")
    
    print("\nä¸€å…ƒæ€§æ‡‰ç”¨:")
    univalence = comprehensive['univalence_applications']
    print(f"  æ½›åœ¨ç­‰åƒ¹: {len(univalence['potential_equivalences'])}")
    print(f"  ç­‰åƒ¹é¡: {len(univalence['equivalence_classes'])}")
    print(f"  èªç¾©è­˜åˆ¥æ•¸: {univalence['semantic_identification_count']}")
    
    print("\nHoTTè¤‡é›œåº¦æŒ‡æ¨™:")
    complexity = comprehensive['hott_complexity_metrics']
    print(f"  é¡å‹è¤‡é›œåº¦: {complexity['type_complexity']:.3f}")
    print(f"  è·¯å¾‘è¤‡é›œåº¦: {complexity['path_complexity']:.3f}")
    print(f"  åŒå€«è¤‡é›œåº¦: {complexity['homotopy_complexity']:.3f}")
    print(f"  æ•´é«”è¤‡é›œåº¦: {complexity['overall_complexity']:.3f}")
    
    return True


def test_integration_with_path_integral():
    """æ¸¬è©¦èˆ‡è·¯å¾‘ç©åˆ†NLPçš„æ•´åˆ"""
    print("\n" + "=" * 60)
    print("èˆ‡è·¯å¾‘ç©åˆ†NLPæ•´åˆæ¸¬è©¦ (Integration with Path Integral NLP Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    # æ¸¬è©¦æ•´åˆèˆ‡éæ•´åˆç‰ˆæœ¬
    hott_nlp_integrated = HomotopyTypeTheoryNLP(integrate_path_integral=True)
    hott_nlp_standalone = HomotopyTypeTheoryNLP(integrate_path_integral=False)
    
    test_text = "ä¾ç…§å¤©é“çš„æŒ‡å¼•ï¼Œé€šéè‡ªç„¶çš„æµå‹•ï¼Œé”åˆ°å’Œè«§çš„å¢ƒç•Œ"
    
    print(f"æ¸¬è©¦æ–‡æœ¬: '{test_text}'")
    print("-" * 40)
    
    print("\n1. æ•´åˆç‰ˆæœ¬åˆ†æ:")
    integrated_analysis = hott_nlp_integrated.construct_path_space_analysis(test_text)
    tian_dao_integrated = integrated_analysis.get('tian_dao_integration', {})
    
    if tian_dao_integrated.get('integration_successful', False):
        print(f"  âœ“ å¤©é“æ•´åˆæˆåŠŸ")
        print(f"  å¤©é“å°é½Šåº¦: {tian_dao_integrated.get('tian_dao_alignment', 0):.3f}")
        print(f"  è‡ªç„¶æµå‹•åˆ†æ•¸: {tian_dao_integrated.get('natural_flow_score', 0):.3f}")
        print(f"  å’Œè«§æŒ‡æ•¸: {tian_dao_integrated.get('harmony_index', 0):.3f}")
    else:
        print(f"  âœ— å¤©é“æ•´åˆå¤±æ•—: {tian_dao_integrated.get('integration_error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    print("\n2. ç¨ç«‹ç‰ˆæœ¬åˆ†æ:")
    standalone_analysis = hott_nlp_standalone.construct_path_space_analysis(test_text)
    tian_dao_standalone = standalone_analysis.get('tian_dao_integration', {})
    
    if not tian_dao_standalone:
        print(f"  âœ“ æŒ‰é æœŸæ²’æœ‰å¤©é“æ•´åˆ")
    else:
        print(f"  æ„å¤–çš„å¤©é“æ•´åˆçµæœ: {tian_dao_standalone}")
    
    print("\n3. HoTT åˆ†ææ¯”è¼ƒ:")
    integrated_hott = integrated_analysis.get('hott_analysis', {})
    standalone_hott = standalone_analysis.get('hott_analysis', {})
    
    print(f"  æ•´åˆç‰ˆè·¯å¾‘æ•¸: {integrated_hott.get('paths_constructed', 0)}")
    print(f"  ç¨ç«‹ç‰ˆè·¯å¾‘æ•¸: {standalone_hott.get('paths_constructed', 0)}")
    print(f"  æ•´åˆç‰ˆåŒå€«æ•¸: {integrated_hott.get('homotopies_constructed', 0)}")
    print(f"  ç¨ç«‹ç‰ˆåŒå€«æ•¸: {standalone_hott.get('homotopies_constructed', 0)}")
    
    return True


def demonstrate_hott_concepts():
    """æ¼”ç¤ºHoTTæ ¸å¿ƒæ¦‚å¿µ"""
    print("\n" + "=" * 60)
    print("åŒå€«é¡å‹è«–æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º (HoTT Core Concepts Demonstration)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¼”ç¤º")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    print("\nğŸ“˜ HoTT æ ¸å¿ƒæ¦‚å¿µ:")
    print("1. é¡å‹å³ç©ºé–“ (Types as Spaces)")
    print("2. é …å³é» (Terms as Points)")  
    print("3. ç­‰å¼å³è·¯å¾‘ (Equality as Paths)")
    print("4. ä¸€å…ƒæ€§åŸç† (Univalence Axiom)")
    print("5. é«˜éšæ­¸ç´é¡å‹ (Higher Inductive Types)")
    
    print("\nğŸ”¬ å¯¦éš›æ‡‰ç”¨ç¤ºä¾‹:")
    
    # 1. é¡å‹å³ç©ºé–“
    print("\n1. é¡å‹ä½œç‚ºèªç¾©ç©ºé–“:")
    emotions_space = hott_nlp.register_semantic_type(
        "emotional_space", 
        ["joy", "sadness", "anger", "peace", "excitement"],
        dimension=3,
        properties={"domain": "emotions", "dimensionality": "high"}
    )
    print(f"   æƒ…æ„Ÿç©ºé–“: {emotions_space.name}, ç¶­åº¦: {emotions_space.dimension}")
    print(f"   ç©ºé–“ä¸­çš„é»: {list(emotions_space.elements)[:3]}...")
    
    # 2. ç­‰å¼å³è·¯å¾‘
    print("\n2. èªç¾©ç­‰å¼ä½œç‚ºè·¯å¾‘:")
    path = hott_nlp.construct_semantic_path("sadness", "peace", HoTTPathType.HOMOTOPY)
    print(f"   è·¯å¾‘: {path.source} â‰¡ {path.target}")
    print(f"   è­‰æ˜é …: {path.proof_term}")
    print(f"   è·¯å¾‘é¡å‹: {path.path_type.value}")
    
    # 3. ä¸€å…ƒæ€§åŸç†
    print("\n3. ä¸€å…ƒæ€§åŸç†æ‡‰ç”¨:")
    type_a = hott_nlp.register_semantic_type("concepts_a", ["æ€æƒ³", "è§€å¿µ", "æƒ³æ³•"])
    type_b = hott_nlp.register_semantic_type("concepts_b", ["æ€ç¶­", "æ¦‚å¿µ", "ç†å¿µ"])
    
    equivalence = hott_nlp.univalence_calculator.construct_equivalence(type_a, type_b)
    if equivalence:
        print(f"   ä¸€å…ƒæ€§ç­‰åƒ¹: {type_a.name} â‰ƒ {type_b.name}")
        print(f"   ç­‰åƒ¹å³ç­‰åŒ: (A â‰ƒ B) â‰ƒ (A = B)")
    else:
        print(f"   é¡å‹ {type_a.name} å’Œ {type_b.name} ä¸ç­‰åƒ¹")
    
    # 4. é«˜éšæ­¸ç´é¡å‹
    print("\n4. é«˜éšæ­¸ç´é¡å‹æ§‹é€ :")
    hit_text = "åœ“çš„æ¦‚å¿µï¼šé»çµ„æˆåœ“å‘¨ï¼Œè·¯å¾‘é€£æ¥é»"
    circle_hit = hott_nlp.construct_higher_inductive_type("circle", hit_text)
    print(f"   HITåç¨±: {circle_hit.name}")
    print(f"   æ§‹é€ å­æ•¸: {len(circle_hit.constructors)}")
    print(f"   è·¯å¾‘æ§‹é€ å­æ•¸: {len(circle_hit.path_constructors)}")
    
    # 5. åŒå€«å±¤æ¬¡
    print("\n5. åŒå€«å±¤æ¬¡åˆ†æ:")
    complex_text = "è¤‡é›œçš„èªç¾©çµæ§‹ï¼ŒåŒ…å«å¤šå±¤å«ç¾©å’Œé—œä¿‚"
    homotopy_analysis = hott_nlp._calculate_homotopy_groups(complex_text)
    print(f"   Ï€â‚€ (é€£é€šåˆ†é‡): {homotopy_analysis['Ï€_0']}")
    print(f"   Ï€â‚ (åŸºæœ¬ç¾¤): {homotopy_analysis['Ï€_1']}")
    
    return True


def test_ethical_action_analysis():
    """æ¸¬è©¦å€«ç†è¡Œå‹•åˆ†æ"""
    print("=" * 60)
    print("å€«ç†è¡Œå‹•åˆ†ææ¸¬è©¦ (Ethical Action Analysis Test)")
    print("=" * 60)
    
    if not HOTT_AVAILABLE:
        print("HomotopyTypeTheoryNLP ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    hott_nlp = HomotopyTypeTheoryNLP()
    
    print("\n1. æ­£é¢å€«ç†èªè¨€æ¸¬è©¦")
    print("-" * 30)
    
    positive_text = "å­¸ç¿’è‹±èªæ˜¯ä¸€å€‹éç¨‹ï¼Œæ¯å€‹äººéƒ½æœ‰ä¸åŒçš„èµ·é»å’Œç™¼å±•é€Ÿåº¦"
    analysis = hott_nlp.ethical_action_analysis(positive_text)
    
    print(f"æ–‡æœ¬: {analysis['text']}")
    print(f"æ¦‚å¿µ: {analysis['concepts']}")
    print(f"å€«ç†è©•åˆ†: {analysis['ethical_evaluation']['total_ethical_score']:.3f}")
    print(f"æ˜¯å¦å¯æ¥å—: {analysis['ethical_evaluation']['is_ethically_acceptable']}")
    print(f"ç¸½é«”è©•ä¼°: {analysis['overall_assessment']}")
    
    if analysis['ethical_recommendations']:
        print(f"æ”¹é€²å»ºè­°: {', '.join(analysis['ethical_recommendations'])}")
    else:
        print("âœ… ç„¡éœ€æ”¹é€²ï¼Œç¬¦åˆå€«ç†æ¨™æº–")
    
    print("\n2. å•é¡Œèªè¨€æª¢æ¸¬æ¸¬è©¦")
    print("-" * 30)
    
    problematic_text = "ä»–çš„è‹±èªå¾ˆå·®ï¼Œç¸½æ˜¯èªªéŒ¯è©±ï¼Œåƒå€‹å¼±æ™ºä¸€æ¨£"
    analysis2 = hott_nlp.ethical_action_analysis(problematic_text)
    
    print(f"æ–‡æœ¬: {analysis2['text']}")
    print(f"å€«ç†è©•åˆ†: {analysis2['ethical_evaluation']['total_ethical_score']:.3f}")
    print(f"æ˜¯å¦å¯æ¥å—: {analysis2['ethical_evaluation']['is_ethically_acceptable']}")
    print(f"ç¸½é«”è©•ä¼°: {analysis2['overall_assessment']}")
    
    if analysis2['ethical_recommendations']:
        print("æ”¹é€²å»ºè­°:")
        for i, rec in enumerate(analysis2['ethical_recommendations'], 1):
            print(f"   {i}. {rec}")
    
    print("\n3. å€«ç†-æ•¸å­¸å°é½Šæ¸¬è©¦")
    print("-" * 30)
    
    complex_text = "èªè¨€å­¸ç¿’æ¶‰åŠèªçŸ¥ã€æƒ…æ„Ÿã€ç¤¾æœƒå¤šå€‹ç¶­åº¦çš„å”èª¿ç™¼å±•"
    analysis3 = hott_nlp.ethical_action_analysis(complex_text)
    
    print(f"æ–‡æœ¬: {analysis3['text']}")
    print(f"å€«ç†-æ•¸å­¸å°é½Šåˆ†æ•¸: {analysis3['ethical_math_alignment']:.3f}")
    print(f"æ•¸å­¸è¤‡é›œåº¦: {analysis3['hott_analysis']['hott_complexity_metrics']['overall_complexity']:.3f}")
    print(f"ç¸½é«”è©•ä¼°: {analysis3['overall_assessment']}")
    
    # é©—è­‰æ¸¬è©¦çµæœ
    success = (
        analysis['ethical_evaluation']['is_ethically_acceptable'] and  # æ­£é¢èªè¨€æ‡‰è©²å¯æ¥å—
        not analysis2['ethical_evaluation']['is_ethically_acceptable'] and  # å•é¡Œèªè¨€æ‡‰è©²ä¸å¯æ¥å—
        len(analysis2['ethical_recommendations']) > 0 and  # å•é¡Œèªè¨€æ‡‰è©²æœ‰å»ºè­°
        analysis3['ethical_math_alignment'] > 0.0  # å°é½Šåˆ†æ•¸æ‡‰è©²å¤§æ–¼0
    )
    
    print(f"\nâœ“ å€«ç†è¡Œå‹•åˆ†ææ¸¬è©¦çµæœ: {'é€šé' if success else 'å¤±æ•—'}")
    return success


def run_all_hott_tests():
    """é‹è¡Œæ‰€æœ‰HoTTæ¸¬è©¦"""
    print("ğŸ§® åŒå€«é¡å‹è«–è‡ªç„¶èªè¨€è™•ç† - å®Œæ•´æ¸¬è©¦å¥—ä»¶")
    print("ğŸ§® Homotopy Type Theory Natural Language Processing - Complete Test Suite")
    print("=" * 80)
    
    if not HOTT_AVAILABLE:
        print("âŒ HomotopyTypeTheoryNLP æ¨¡çµ„ä¸å¯ç”¨ï¼Œç„¡æ³•é‹è¡Œæ¸¬è©¦")
        return False
    
    test_results = []
    
    try:
        # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
        print("\nğŸ” é–‹å§‹æ¸¬è©¦...")
        
        test_results.append(("åŸºæœ¬HoTTæ¦‚å¿µ", test_basic_hott_concepts()))
        test_results.append(("è·¯å¾‘ç©ºé–“åˆ†æ", test_path_space_analysis()))
        test_results.append(("ä¸€å…ƒæ€§æ‡‰ç”¨", test_univalence_applications()))
        test_results.append(("é«˜éšæ­¸ç´é¡å‹", test_higher_inductive_types()))
        test_results.append(("åŒå€«åˆ†æ", test_homotopy_analysis()))
        test_results.append(("ç¶œåˆåˆ†æ", test_comprehensive_analysis()))
        test_results.append(("è·¯å¾‘ç©åˆ†æ•´åˆ", test_integration_with_path_integral()))
        test_results.append(("å€«ç†è¡Œå‹•åˆ†æ", test_ethical_action_analysis()))
        test_results.append(("æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º", demonstrate_hott_concepts()))
        
        print("\n" + "=" * 80)
        print("ğŸ¯ æ¸¬è©¦çµæœç¸½çµ (Test Results Summary)")
        print("=" * 80)
        
        passed_tests = 0
        failed_tests = 0
        
        for test_name, result in test_results:
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"{status} {test_name}")
            if result:
                passed_tests += 1
            else:
                failed_tests += 1
        
        print(f"\nğŸ“Š çµ±è¨ˆ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {len(test_results)}")
        print(f"   é€šé: {passed_tests}")
        print(f"   å¤±æ•—: {failed_tests}")
        print(f"   æˆåŠŸç‡: {passed_tests/len(test_results)*100:.1f}%")
        
        if failed_tests == 0:
            print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼åŒå€«é¡å‹è«–NLPç³»çµ±é‹è¡Œæ­£å¸¸ã€‚")
            print("ğŸ‰ All tests passed! HoTT NLP system is working correctly.")
            
            print("\nğŸ”¬ ç³»çµ±ç‰¹æ€§ç¸½çµ:")
            print("   âœ“ åŸºæœ¬HoTTæ¦‚å¿µå¯¦ç¾ (é¡å‹ã€è·¯å¾‘ã€ç­‰åƒ¹)")
            print("   âœ“ ä¸€å…ƒæ€§åŸç†æ‡‰ç”¨æ–¼èªç¾©åˆ†æ")
            print("   âœ“ é«˜éšæ­¸ç´é¡å‹æ§‹é€ èªè¨€çµæ§‹")
            print("   âœ“ åŒå€«åˆ†ææª¢æ¸¬èªç¾©ç­‰åƒ¹æ€§")
            print("   âœ“ èˆ‡å¤©é“è·¯å¾‘ç©åˆ†NLPå®Œç¾æ•´åˆ")
            print("   âœ“ æ”¯æŒä¸­è‹±æ–‡æ··åˆèªç¾©åˆ†æ")
            print("   âœ“ å€«ç†è¡Œå‹•åˆ†æç¢ºä¿èªè¨€ä½¿ç”¨çš„é“å¾·æ€§")
            print("   âœ“ æä¾›å®Œæ•´çš„æ•¸å­¸åŸºç¤æ¡†æ¶")
            
        else:
            print(f"\nâš ï¸  æœ‰ {failed_tests} å€‹æ¸¬è©¦å¤±æ•—ï¼Œç³»çµ±éœ€è¦é€²ä¸€æ­¥èª¿è©¦")
        
        return failed_tests == 0
        
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("ğŸ”® åŒå€«é¡å‹è«–è‡ªç„¶èªè¨€è™•ç† - æ¸¬è©¦èˆ‡æ¼”ç¤º")
    print("ğŸ”® Homotopy Type Theory Natural Language Processing - Tests & Examples")
    print("=" * 80)
    
    # æª¢æŸ¥ç’°å¢ƒ
    if HOTT_AVAILABLE:
        print("âœ… HomotopyTypeTheoryNLP æ¨¡çµ„å¯ç”¨")
        print("ğŸ”— æ”¯æŒèˆ‡PathIntegralNLPæ•´åˆ")
    else:
        print("âŒ HomotopyTypeTheoryNLP æ¨¡çµ„ä¸å¯ç”¨")
    
    # é‹è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
    success = run_all_hott_tests()
    
    if success:
        print("\n" + "=" * 80)
        print("ğŸŒŸ åŒå€«é¡å‹è«–NLPç³»çµ±å·²å®Œå…¨å°±ç·’ï¼")
        print("ğŸŒŸ Homotopy Type Theory NLP System is fully operational!")
        print("ğŸ”¬ ç¾åœ¨æ‚¨å¯ä»¥ä½¿ç”¨HoTTçš„æ•¸å­¸åš´è¬¹æ€§ä¾†åˆ†æè‡ªç„¶èªè¨€")
        print("ğŸ”¬ You can now analyze natural language with HoTT's mathematical rigor")
        print("=" * 80)
    else:
        print("\n" + "=" * 80)
        print("ğŸ”§ ç³»çµ±éœ€è¦é€²ä¸€æ­¥èª¿è©¦å’Œå®Œå–„")
        print("ğŸ”§ System requires further debugging and refinement")
        print("=" * 80)