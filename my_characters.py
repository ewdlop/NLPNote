#!/usr/bin/env python3
"""
æˆ‘çš„å­—ç¬¦ (My Characters) - ç¶œåˆå­—ç¬¦åˆ†æå·¥å…·
Comprehensive Character Analysis Tool

This is the main "My Characters" tool that integrates character analysis,
transformation, and human expression evaluation for the multilingual NLP repository.

é€™æ˜¯ä¸»è¦çš„"æˆ‘çš„å­—ç¬¦"å·¥å…·ï¼Œæ•´åˆäº†å­—ç¬¦åˆ†æã€è®Šæ›å’Œäººé¡è¡¨é”è©•ä¼°åŠŸèƒ½ï¼Œ
ç”¨æ–¼å¤šèªè¨€NLPå­˜å„²åº«ã€‚
"""

import sys
import os
import argparse
import json
from typing import Dict, List, Any, Optional
from datetime import datetime

# Import our custom modules
from CharacterAnalyzer import CharacterAnalyzer, WritingSystem, CharacterStats
from CharacterTransformer import CharacterTransformer, TransformationResult
from HumanExpressionEvaluator import HumanExpressionEvaluator, ExpressionContext


class MyCharacters:
    """
    æˆ‘çš„å­—ç¬¦ä¸»é¡ (My Characters Main Class)
    
    Comprehensive character analysis and transformation system
    for multilingual NLP applications.
    """
    
    def __init__(self):
        self.analyzer = CharacterAnalyzer()
        self.transformer = CharacterTransformer()
        self.expression_evaluator = HumanExpressionEvaluator()
        self.repository_analyzed = False
        self.analysis_cache = {}
        
    def initialize_repository_analysis(self, force_refresh: bool = False):
        """åˆå§‹åŒ–å­˜å„²åº«åˆ†æ (Initialize repository analysis)"""
        if self.repository_analyzed and not force_refresh:
            return
            
        print("ğŸ” åˆ†æå­˜å„²åº«å­—ç¬¦... (Analyzing repository characters...)")
        try:
            # Analyze the repository
            results = self.analyzer.analyze_directory(".", ['.md', '.py', '.txt', '.json', '.html'])
            self.analysis_cache = results
            self.repository_analyzed = True
            
            print(f"âœ“ åˆ†æå®Œæˆ! ç™¼ç¾ {results['unique_characters']} å€‹å”¯ä¸€å­—ç¬¦")
            print(f"âœ“ Analysis complete! Found {results['unique_characters']} unique characters")
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±æ•—: {e}")
            print(f"âŒ Analysis failed: {e}")
    
    def show_comprehensive_summary(self):
        """é¡¯ç¤ºç¶œåˆç¸½çµ (Show comprehensive summary)"""
        if not self.repository_analyzed:
            self.initialize_repository_analysis()
        
        print("\n" + "="*80)
        print("ğŸ“Š æˆ‘çš„å­—ç¬¦ - ç¶œåˆåˆ†æå ±å‘Š (My Characters - Comprehensive Analysis)")
        print("="*80)
        
        # Basic statistics
        results = self.analysis_cache
        print(f"ğŸ“ åˆ†ææ–‡ä»¶æ•¸ (Files analyzed): {results['total_files']}")
        print(f"ğŸ“ ç¸½å­—ç¬¦æ•¸ (Total characters): {results['total_characters']:,}")
        print(f"ğŸ”¤ å”¯ä¸€å­—ç¬¦æ•¸ (Unique characters): {results['unique_characters']:,}")
        
        # Writing system distribution
        print(f"\nğŸ“š æ›¸å¯«ç³»çµ±åˆ†å¸ƒ (Writing System Distribution):")
        ws_counts = results['writing_systems']
        total_ws_chars = sum(ws_counts.values())
        
        for ws, count in sorted(ws_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_ws_chars * 100) if total_ws_chars > 0 else 0
            ws_name = ws.value if hasattr(ws, 'value') else str(ws)
            icon = self._get_writing_system_icon(ws)
            bar = "â–ˆ" * min(int(percentage / 2), 40)
            print(f"  {icon} {ws_name:12} {count:6,} ({percentage:5.1f}%) {bar}")
        
        # Repository character insights
        self._show_repository_insights()
        
        # Top multilingual expressions
        self._show_multilingual_expressions()
    
    def _get_writing_system_icon(self, ws) -> str:
        """ç²å–æ›¸å¯«ç³»çµ±åœ–æ¨™ (Get writing system icon)"""
        if isinstance(ws, str):
            try:
                ws = WritingSystem(ws)
            except:
                return "â“"
        
        icons = {
            WritingSystem.LATIN: "ğŸ”¤",
            WritingSystem.CJK: "ğŸ€„",
            WritingSystem.ARABIC: "ğŸ”—",
            WritingSystem.HEBREW: "ğŸ”¯",
            WritingSystem.CYRILLIC: "ğŸ‡·ğŸ‡º",
            WritingSystem.GREEK: "ğŸ‡¬ğŸ‡·",
            WritingSystem.DEVANAGARI: "ğŸ‡®ğŸ‡³",
            WritingSystem.PUNCTUATION: "â—",
            WritingSystem.SYMBOLS: "ğŸ”£",
            WritingSystem.DIGITS: "ğŸ”¢",
            WritingSystem.OTHER: "â“"
        }
        return icons.get(ws, "â“")
    
    def _show_repository_insights(self):
        """é¡¯ç¤ºå­˜å„²åº«æ´å¯Ÿ (Show repository insights)"""
        print(f"\nğŸ” å­˜å„²åº«å­—ç¬¦æ´å¯Ÿ (Repository Character Insights):")
        
        # Most common characters by writing system
        ws_top_chars = {}
        for char, freq in self.analyzer.character_frequency.most_common(100):
            if not char.isspace():
                char_info = self.analyzer.get_character_info(char)
                ws = char_info.writing_system
                if ws not in ws_top_chars:
                    ws_top_chars[ws] = []
                if len(ws_top_chars[ws]) < 3:
                    ws_top_chars[ws].append((char, freq))
        
        for ws, chars in ws_top_chars.items():
            if chars:
                icon = self._get_writing_system_icon(ws)
                ws_name = ws.value if hasattr(ws, 'value') else str(ws)
                char_list = ', '.join([f"'{c}' ({f:,})" for c, f in chars])
                print(f"  {icon} {ws_name}: {char_list}")
    
    def _show_multilingual_expressions(self):
        """é¡¯ç¤ºå¤šèªè¨€è¡¨é”å¼ (Show multilingual expressions)"""
        print(f"\nğŸŒ å¤šèªè¨€è¡¨é”ç¯„ä¾‹ (Multilingual Expression Examples):")
        
        # Find files with interesting multilingual content
        sample_expressions = [
            "æˆ‘çš„å­—ç¬¦ (My Characters)",
            "Human Expression Evaluation",
            "è‡ªç„¶èªè¨€è™•ç† (Natural Language Processing)",
            "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸",
            "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…"
        ]
        
        for expr in sample_expressions:
            if any(char in self.analyzer.character_frequency for char in expr):
                char_analysis = self.analyzer.analyze_text(expr)
                ws_list = list(char_analysis['writing_systems'].keys())
                ws_names = [ws.value if hasattr(ws, 'value') else str(ws) for ws in ws_list]
                print(f"  ğŸ“ '{expr}' - {', '.join(ws_names)}")
    
    def analyze_custom_expression(self, expression: str, include_transformation: bool = True):
        """åˆ†æè‡ªå®šç¾©è¡¨é”å¼ (Analyze custom expression)"""
        print(f"\nğŸ“ åˆ†æè¡¨é”å¼: '{expression}'")
        print("="*60)
        
        # Character analysis
        char_analysis = self.analyzer.analyze_text(expression, "user_input")
        print(f"å­—ç¬¦ç¸½æ•¸ (Total chars): {char_analysis['total_characters']}")
        print(f"å”¯ä¸€å­—ç¬¦æ•¸ (Unique chars): {char_analysis['unique_characters']}")
        
        # Character details
        print(f"\nğŸ”¤ å­—ç¬¦è©³æƒ… (Character Details):")
        for char, char_info in char_analysis['character_infos'].items():
            icon = self._get_writing_system_icon(char_info.writing_system)
            print(f"  '{char}' {icon} U+{char_info.unicode_codepoint:04X} - {char_info.unicode_name}")
        
        # Writing system distribution
        print(f"\nğŸ“š æ›¸å¯«ç³»çµ±åˆ†å¸ƒ (Writing Systems):")
        for ws, count in char_analysis['writing_systems'].items():
            icon = self._get_writing_system_icon(ws)
            ws_name = ws.value if hasattr(ws, 'value') else str(ws)
            print(f"  {icon} {ws_name}: {count}")
        
        # Human expression evaluation
        try:
            print(f"\nğŸ§  äººé¡è¡¨é”è©•ä¼° (Human Expression Evaluation):")
            context = ExpressionContext()
            
            # Get individual evaluations
            formal_result = self.expression_evaluator.formal_evaluator.evaluate(expression)
            cognitive_result = self.expression_evaluator.cognitive_evaluator.evaluate_expression(expression, context)
            social_result = self.expression_evaluator.social_evaluator.evaluate_social_expression(expression, "user", context)
            
            print(f"  å½¢å¼èªç¾© (Formal): {formal_result.score:.3f}")
            print(f"  èªçŸ¥è™•ç† (Cognitive): {cognitive_result.score:.3f}")
            print(f"  ç¤¾æœƒé©ç•¶ (Social): {social_result.score:.3f}")
            
        except Exception as e:
            print(f"  âš ï¸ è©•ä¼°éŒ¯èª¤: {e}")
        
        # Character transformation examples
        if include_transformation:
            print(f"\nğŸ”„ å­—ç¬¦è®Šæ›ç¯„ä¾‹ (Character Transformations):")
            
            target_systems = [WritingSystem.CJK, WritingSystem.CYRILLIC, WritingSystem.ARABIC]
            for target in target_systems:
                try:
                    result = self.transformer.transform_text(expression, target, 'random')
                    icon = self._get_writing_system_icon(target)
                    print(f"  {icon} {target.value}: {result.transformed_text}")
                except Exception as e:
                    print(f"  âŒ {target.value}: è®Šæ›å¤±æ•— ({e})")
    
    def generate_character_report(self, output_file: str = "my_characters_full_report.md"):
        """ç”Ÿæˆå®Œæ•´å­—ç¬¦å ±å‘Š (Generate comprehensive character report)"""
        if not self.repository_analyzed:
            self.initialize_repository_analysis()
        
        print(f"ğŸ“„ ç”Ÿæˆå ±å‘Š... (Generating report...)")
        
        report_lines = [
            "# æˆ‘çš„å­—ç¬¦ - å®Œæ•´åˆ†æå ±å‘Š (My Characters - Full Analysis Report)",
            "",
            f"ç”Ÿæˆæ™‚é–“ (Generated): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ” ç¸½è¦½ (Overview)",
            f"- åˆ†ææ–‡ä»¶æ•¸: {self.analysis_cache['total_files']}",
            f"- ç¸½å­—ç¬¦æ•¸: {self.analysis_cache['total_characters']:,}",
            f"- å”¯ä¸€å­—ç¬¦æ•¸: {self.analysis_cache['unique_characters']:,}",
            "",
            "## ğŸ“š æ›¸å¯«ç³»çµ±è©³ç´°åˆ†æ (Writing System Analysis)",
            ""
        ]
        
        # Detailed writing system analysis
        ws_counts = self.analysis_cache['writing_systems']
        total_ws_chars = sum(ws_counts.values())
        
        for ws, count in sorted(ws_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_ws_chars * 100) if total_ws_chars > 0 else 0
            ws_name = ws.value if hasattr(ws, 'value') else str(ws)
            icon = self._get_writing_system_icon(ws)
            
            report_lines.extend([
                f"### {icon} {ws_name}",
                f"- å­—ç¬¦æ•¸: {count:,} ({percentage:.1f}%)",
                f"- ç¯„ä¾‹å­—ç¬¦: {self._get_sample_characters(ws, 10)}",
                ""
            ])
        
        # Character frequency analysis
        report_lines.extend([
            "## ğŸ† å­—ç¬¦é »ç‡åˆ†æ (Character Frequency Analysis)",
            "",
            "### æœ€å¸¸ç”¨å­—ç¬¦ (Top Characters)",
            ""
        ])
        
        for i, (char, freq) in enumerate(self.analyzer.character_frequency.most_common(50), 1):
            if not char.isspace():
                char_info = self.analyzer.get_character_info(char)
                icon = self._get_writing_system_icon(char_info.writing_system)
                report_lines.append(
                    f"{i:2d}. '{char}' {icon} U+{char_info.unicode_codepoint:04X} "
                    f"({freq:,} times) - {char_info.unicode_name}"
                )
        
        # File analysis summary
        report_lines.extend([
            "",
            "## ğŸ“ æ–‡ä»¶åˆ†ææ‘˜è¦ (File Analysis Summary)",
            ""
        ])
        
        # Show top 10 files by character count
        file_results = self.analysis_cache.get('file_results', {})
        sorted_files = sorted(
            [(f, r) for f, r in file_results.items() if 'total_characters' in r],
            key=lambda x: x[1]['total_characters'],
            reverse=True
        )
        
        for file_path, result in sorted_files[:10]:
            ws_count = len(result.get('writing_systems', {}))
            report_lines.append(
                f"- `{file_path}`: {result['total_characters']:,} å­—ç¬¦, "
                f"{result['unique_characters']} å”¯ä¸€å­—ç¬¦, {ws_count} æ›¸å¯«ç³»çµ±"
            )
        
        # Encoding issues
        encoding_issues = self.analysis_cache.get('encoding_issues', [])
        if encoding_issues:
            report_lines.extend([
                "",
                "## âš ï¸  ç·¨ç¢¼å•é¡Œ (Encoding Issues)",
                ""
            ])
            for issue in encoding_issues:
                report_lines.append(f"- {issue}")
        
        # Write report
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ“ å ±å‘Šå·²ä¿å­˜åˆ° {output_file}")
        print(f"âœ“ Report saved to {output_file}")
    
    def _get_sample_characters(self, writing_system, limit: int = 5) -> str:
        """ç²å–æ›¸å¯«ç³»çµ±çš„ç¯„ä¾‹å­—ç¬¦ (Get sample characters for writing system)"""
        samples = []
        for char, freq in self.analyzer.character_frequency.most_common():
            if len(samples) >= limit:
                break
            if not char.isspace():
                char_info = self.analyzer.get_character_info(char)
                if char_info.writing_system == writing_system:
                    samples.append(f"'{char}'")
        return ', '.join(samples) if samples else "ç„¡ (None)"
    
    def export_data(self, format_type: str = 'json', output_file: str = None):
        """å°å‡ºæ•¸æ“š (Export data)"""
        if not self.repository_analyzed:
            self.initialize_repository_analysis()
        
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"my_characters_data_{timestamp}.{format_type}"
        
        print(f"ğŸ“¤ å°å‡ºæ•¸æ“š... (Exporting data...)")
        
        # Prepare export data
        export_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_files': self.analysis_cache['total_files'],
                'total_characters': self.analysis_cache['total_characters'],
                'unique_characters': self.analysis_cache['unique_characters']
            },
            'writing_systems': {
                str(ws): count for ws, count in self.analysis_cache['writing_systems'].items()
            },
            'character_frequency': [
                {
                    'character': char,
                    'frequency': freq,
                    'unicode': f"U+{ord(char):04X}",
                    'name': self.analyzer.get_character_info(char).unicode_name,
                    'writing_system': str(self.analyzer.get_character_info(char).writing_system)
                }
                for char, freq in self.analyzer.character_frequency.most_common()
                if not char.isspace()
            ]
        }
        
        try:
            if format_type == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                    
            elif format_type == 'csv':
                import pandas as pd
                # Create character dataframe
                df = self.analyzer.create_character_dataframe()
                df.to_csv(output_file, index=False, encoding='utf-8')
                
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
                return
            
            print(f"âœ“ æ•¸æ“šå·²å°å‡ºåˆ° {output_file}")
            print(f"âœ“ Data exported to {output_file}")
            
        except Exception as e:
            print(f"âŒ å°å‡ºå¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸ (Main function)"""
    parser = argparse.ArgumentParser(
        description="æˆ‘çš„å­—ç¬¦ - ç¶œåˆå­—ç¬¦åˆ†æå·¥å…· (My Characters - Comprehensive Character Analysis Tool)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ç”¨æ³• (Example usage):
  python3 my_characters.py --summary          # é¡¯ç¤ºç¶œåˆç¸½çµ
  python3 my_characters.py --analyze "æ¸¬è©¦"   # åˆ†æç‰¹å®šè¡¨é”å¼
  python3 my_characters.py --report           # ç”Ÿæˆå®Œæ•´å ±å‘Š
  python3 my_characters.py --export json      # å°å‡ºJSONæ•¸æ“š
        """
    )
    
    parser.add_argument('--summary', action='store_true', 
                       help='é¡¯ç¤ºç¶œåˆå­—ç¬¦åˆ†æç¸½çµ')
    parser.add_argument('--analyze', type=str, 
                       help='åˆ†æç‰¹å®šè¡¨é”å¼çš„å­—ç¬¦')
    parser.add_argument('--report', action='store_true', 
                       help='ç”Ÿæˆå®Œæ•´çš„å­—ç¬¦åˆ†æå ±å‘Š')
    parser.add_argument('--export', choices=['json', 'csv'], 
                       help='å°å‡ºæ•¸æ“š (jsonæˆ–csvæ ¼å¼)')
    parser.add_argument('--output', type=str, 
                       help='è¼¸å‡ºæ–‡ä»¶å')
    parser.add_argument('--force-refresh', action='store_true', 
                       help='å¼·åˆ¶é‡æ–°åˆ†æå­˜å„²åº«')
    
    args = parser.parse_args()
    
    # Create My Characters instance
    my_chars = MyCharacters()
    
    # Initialize if needed
    if any([args.summary, args.analyze, args.report, args.export]):
        my_chars.initialize_repository_analysis(args.force_refresh)
    
    # Execute commands
    if args.summary:
        my_chars.show_comprehensive_summary()
    
    if args.analyze:
        my_chars.analyze_custom_expression(args.analyze)
    
    if args.report:
        output_file = args.output or "my_characters_full_report.md"
        my_chars.generate_character_report(output_file)
    
    if args.export:
        my_chars.export_data(args.export, args.output)
    
    # Default behavior: show summary
    if len(sys.argv) == 1:
        print("ğŸš€ æ­¡è¿ä½¿ç”¨æˆ‘çš„å­—ç¬¦åˆ†æå·¥å…·ï¼")
        print("ğŸš€ Welcome to My Characters Analysis Tool!")
        print("\nä½¿ç”¨ --help æŸ¥çœ‹æ‰€æœ‰é¸é …")
        print("Use --help to see all options")
        my_chars.show_comprehensive_summary()


if __name__ == "__main__":
    main()