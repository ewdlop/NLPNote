#!/usr/bin/env python3
"""
é™ç¶­æ‰“æ“Šå’ŒFlattening Earth å¯¦ä¾‹æ¼”ç¤º
Dimensional Reduction Attack and Flattening Earth Examples

é€™å€‹æ–‡ä»¶æä¾›äº†ä½¿ç”¨é™ç¶­æ”»æ“Šå’Œåœ°çƒæ‰å¹³åŒ–åŠŸèƒ½çš„å¯¦éš›ä¾‹å­ã€‚
This file provides practical examples of using dimensional reduction attack and earth flattening functionality.

Usage:
    python dimensional_reduction_examples.py
"""

import numpy as np
import sys
import os

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from DimensionalReductionAttack import (
        DimensionalAttackOrchestrator, 
        EarthFlattener,
        generate_sample_high_dimensional_data
    )
    MAIN_MODULE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import main module: {e}")
    MAIN_MODULE_AVAILABLE = False


def example_1_word_embedding_attack():
    """ç¤ºä¾‹1: è©å‘é‡é™ç¶­æ”»æ“Š"""
    print("\n" + "="*60)
    print("ğŸ¯ ç¤ºä¾‹1: è©å‘é‡é™ç¶­æ”»æ“Š (Word Embedding Dimensional Attack)")
    print("="*60)
    
    if not MAIN_MODULE_AVAILABLE:
        print("Main module not available, skipping example")
        return
    
    # æ¨¡æ“¬300ç¶­çš„è©å‘é‡æ•¸æ“š
    print("1. ç”Ÿæˆæ¨¡æ“¬è©å‘é‡æ•¸æ“š...")
    vocabulary_size = 1000
    embedding_dim = 300
    word_embeddings = generate_sample_high_dimensional_data(vocabulary_size, embedding_dim)
    
    print(f"   è©å½™è¡¨å¤§å°: {vocabulary_size}")
    print(f"   åŸå§‹å‘é‡ç¶­åº¦: {embedding_dim}")
    
    # å‰µå»ºæ”»æ“Šå”èª¿å™¨
    orchestrator = DimensionalAttackOrchestrator()
    
    # åŸ·è¡Œä¸åŒçš„é™ç¶­æ”»æ“Š
    print("\n2. åŸ·è¡Œé™ç¶­æ”»æ“Š...")
    
    # PCAæ”»æ“Š - é™ç¶­åˆ°50ç¶­ç”¨æ–¼ç‰¹å¾µæå–
    if 'pca' in orchestrator.list_available_attacks():
        pca_result = orchestrator.execute_dimensional_attack(
            data=word_embeddings,
            attack_method='pca',
            target_dimensions=50
        )
        print(f"   PCAæ”»æ“Šå®Œæˆ: ä¿ç•™ {pca_result.data_preserved_ratio:.1%} çš„ä¿¡æ¯")
    
    # t-SNEæ”»æ“Š - é™ç¶­åˆ°2ç¶­ç”¨æ–¼å¯è¦–åŒ–
    if 'tsne' in orchestrator.list_available_attacks():
        tsne_result = orchestrator.execute_dimensional_attack(
            data=word_embeddings,
            attack_method='tsne', 
            target_dimensions=2
        )
        print(f"   t-SNEæ”»æ“Šå®Œæˆ: ç”Ÿæˆ2ç¶­å¯è¦–åŒ–è¡¨ç¤º")
    
    print("\nâœ… è©å‘é‡é™ç¶­æ”»æ“Šç¤ºä¾‹å®Œæˆ!")


def example_2_document_feature_attack():
    """ç¤ºä¾‹2: æ–‡æª”ç‰¹å¾µé™ç¶­æ”»æ“Š"""
    print("\n" + "="*60)
    print("ğŸ“š ç¤ºä¾‹2: æ–‡æª”ç‰¹å¾µé™ç¶­æ”»æ“Š (Document Feature Dimensional Attack)")
    print("="*60)
    
    if not MAIN_MODULE_AVAILABLE:
        print("Main module not available, skipping example")
        return
    
    # æ¨¡æ“¬TF-IDFç‰¹å¾µçŸ©é™£
    print("1. ç”Ÿæˆæ¨¡æ“¬TF-IDFç‰¹å¾µ...")
    n_documents = 500
    vocabulary_size = 10000
    tfidf_features = np.random.rand(n_documents, vocabulary_size) * 0.1  # ç¨€ç–ç‰¹å¾µ
    
    print(f"   æ–‡æª”æ•¸é‡: {n_documents}")
    print(f"   è©å½™è¡¨å¤§å°: {vocabulary_size}")
    print(f"   ç‰¹å¾µçŸ©é™£å½¢ç‹€: {tfidf_features.shape}")
    
    # åŸ·è¡Œæ”»æ“Š
    orchestrator = DimensionalAttackOrchestrator()
    
    print("\n2. åŸ·è¡Œç‰¹å¾µé™ç¶­æ”»æ“Š...")
    
    # ä½¿ç”¨PCAé™ç¶­åˆ°æ›´åˆç†çš„ç¶­åº¦
    if 'pca' in orchestrator.list_available_attacks():
        feature_result = orchestrator.execute_dimensional_attack(
            data=tfidf_features,
            attack_method='pca',
            target_dimensions=100
        )
        
        print(f"   åŸå§‹ç‰¹å¾µ: {feature_result.original_dimensions}ç¶­")
        print(f"   æ”»æ“Šå¾Œç‰¹å¾µ: {feature_result.reduced_dimensions}ç¶­")
        print(f"   å£“ç¸®æ¯”ä¾‹: {feature_result.attack_effectiveness:.1%}")
        print(f"   ä¿¡æ¯ä¿ç•™: {feature_result.data_preserved_ratio:.1%}")
    
    print("\nâœ… æ–‡æª”ç‰¹å¾µé™ç¶­æ”»æ“Šç¤ºä¾‹å®Œæˆ!")


def example_3_earth_flattening():
    """ç¤ºä¾‹3: åœ°çƒæ‰å¹³åŒ–æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸŒ ç¤ºä¾‹3: åœ°çƒæ‰å¹³åŒ–æ¼”ç¤º (Earth Flattening Demonstration)")
    print("="*60)
    
    if not MAIN_MODULE_AVAILABLE:
        print("Main module not available, skipping example")
        return
    
    flattener = EarthFlattener()
    
    # 3.1 è¤‡é›œé…ç½®æ–‡ä»¶æ‰å¹³åŒ–
    print("\n3.1 è¤‡é›œé…ç½®æ–‡ä»¶æ‰å¹³åŒ–...")
    
    complex_config = {
        'application': {
            'name': 'NLP_System',
            'version': '1.0.0',
            'database': {
                'primary': {
                    'host': 'localhost',
                    'port': 5432,
                    'credentials': {
                        'username': 'nlp_user',
                        'password': 'secure_pass',
                        'auth_method': 'password'
                    }
                },
                'backup': {
                    'host': 'backup.example.com',
                    'port': 5433
                }
            },
            'api': {
                'endpoints': {
                    'v1': {
                        'users': '/api/v1/users',
                        'documents': '/api/v1/documents',
                        'analysis': '/api/v1/analysis'
                    },
                    'v2': {
                        'users': '/api/v2/users',
                        'ml_models': '/api/v2/models'
                    }
                },
                'rate_limiting': {
                    'requests_per_minute': 1000,
                    'burst_limit': 1500
                }
            }
        }
    }
    
    config_result = flattener.flatten_nested_dict(complex_config, separator='.')
    
    print(f"   åŸå§‹é…ç½®å±¤ç´šæ·±åº¦: {config_result.metadata['original_depth']}")
    print(f"   æ‰å¹³åŒ–å¾Œéµæ•¸é‡: {config_result.metadata['flattened_keys']}")
    print(f"   è¤‡é›œåº¦æ¸›å°‘: {config_result.complexity_reduction:.1%}")
    
    print("\n   éƒ¨åˆ†æ‰å¹³åŒ–çµæœ:")
    for i, (key, value) in enumerate(list(config_result.flattened_structure.items())[:5]):
        print(f"   {key}: {value}")
    print("   ...")
    
    # 3.2 åµŒå¥—æ•¸æ“šçµæ§‹æ‰å¹³åŒ–
    print("\n3.2 åµŒå¥—æ•¸æ“šçµæ§‹æ‰å¹³åŒ–...")
    
    nested_data = [
        [1, 2, [3, 4]],
        [5, [6, [7, 8, 9]]],
        [[10, 11], 12],
        [13, [14, [15, [16]]]]
    ]
    
    list_result = flattener.flatten_nested_list(nested_data)
    
    print(f"   åŸå§‹åˆ—è¡¨: {nested_data}")
    print(f"   æ‰å¹³åŒ–å¾Œ: {list_result.flattened_structure}")
    print(f"   åŸå§‹æ·±åº¦: {list_result.metadata['original_depth']}")
    print(f"   è¤‡é›œåº¦æ¸›å°‘: {list_result.complexity_reduction:.1%}")
    
    # 3.3 è¤‡é›œèªè¨€çµæ§‹æ‰å¹³åŒ–
    print("\n3.3 è¤‡é›œèªè¨€çµæ§‹æ‰å¹³åŒ–...")
    
    complex_text = """
    Natural language processing, which encompasses a wide variety of computational linguistics techniques, 
    machine learning algorithms, and artificial intelligence methodologies, has become increasingly important 
    in the modern era of big data and digital transformation, particularly in applications such as sentiment 
    analysis, machine translation, information extraction, and question-answering systems that require 
    sophisticated understanding of human language nuances, contextual meanings, and semantic relationships.
    """
    
    text_result = flattener.flatten_linguistic_structure(complex_text.strip(), max_sentence_length=15)
    
    print(f"   åŸå§‹æ–‡æœ¬é•·åº¦: {len(complex_text.strip())} å­—ç¬¦")
    print(f"   åŸå§‹å¥å­æ•¸: {text_result.metadata['original_sentences']}")
    print(f"   æ‰å¹³åŒ–å¾Œå¥å­æ•¸: {text_result.metadata['flattened_sentences']}")
    print(f"   è¤‡é›œåº¦æ¸›å°‘: {text_result.complexity_reduction:.1%}")
    print(f"   ä¿¡æ¯æå¤±ä¼°è¨ˆ: {text_result.information_loss:.1%}")
    
    print(f"\n   æ‰å¹³åŒ–å¾Œæ–‡æœ¬:\n   {text_result.flattened_structure}")
    
    print("\nâœ… åœ°çƒæ‰å¹³åŒ–æ¼”ç¤ºå®Œæˆ!")


def example_4_combined_attack():
    """ç¤ºä¾‹4: çµ„åˆæ”»æ“Šæ¼”ç¤º"""
    print("\n" + "="*60)
    print("âš¡ ç¤ºä¾‹4: çµ„åˆæ”»æ“Šæ¼”ç¤º (Combined Attack Demonstration)")
    print("="*60)
    
    if not MAIN_MODULE_AVAILABLE:
        print("Main module not available, skipping example")
        return
    
    # å‰µå»ºå¤šå±¤æ¬¡çš„è¤‡é›œæ•¸æ“šçµæ§‹
    print("1. å‰µå»ºè¤‡é›œæ•¸æ“šçµæ§‹...")
    
    # é«˜ç¶­æ•¸æ“š
    high_dim_data = generate_sample_high_dimensional_data(n_samples=200, n_dimensions=30)
    
    # è¤‡é›œåµŒå¥—çµæ§‹
    nested_structure = {
        'experiment_data': {
            'features': high_dim_data.tolist(),  # è½‰æ›ç‚ºåˆ—è¡¨ä¾¿æ–¼åºåˆ—åŒ–
            'metadata': {
                'dimensions': high_dim_data.shape[1],
                'samples': high_dim_data.shape[0],
                'creation_time': '2024-12-22',
                'parameters': {
                    'algorithm': 'multivariate_normal',
                    'clusters': 3,
                    'noise_level': 0.1
                }
            }
        }
    }
    
    print(f"   æ•¸æ“šç¶­åº¦: {high_dim_data.shape}")
    print(f"   åµŒå¥—çµæ§‹å±¤ç´š: æ·±å±¤åµŒå¥—")
    
    # åŸ·è¡Œçµ„åˆæ”»æ“Š
    print("\n2. åŸ·è¡Œçµ„åˆæ”»æ“Š...")
    
    orchestrator = DimensionalAttackOrchestrator()
    flattener = EarthFlattener()
    
    # ç¬¬ä¸€éšæ®µ: é™ç¶­æ”»æ“Š
    dimensional_results = orchestrator.execute_combined_attack(high_dim_data)
    
    # ç¬¬äºŒéšæ®µ: çµæ§‹æ‰å¹³åŒ–  
    structure_result = flattener.flatten_nested_dict(nested_structure)
    
    print(f"\n3. æ”»æ“Šçµæœæ‘˜è¦:")
    print(f"   ç¶­åº¦æ”»æ“Šæ–¹æ³•æ•¸: {len(dimensional_results)}")
    print(f"   çµæ§‹è¤‡é›œåº¦æ¸›å°‘: {structure_result.complexity_reduction:.1%}")
    
    # è¨ˆç®—ç¸½é«”æ”»æ“Šæ•ˆæœ
    if dimensional_results:
        avg_attack_effectiveness = np.mean([
            result.attack_effectiveness for result in dimensional_results.values()
        ])
        avg_data_preservation = np.mean([
            result.data_preserved_ratio for result in dimensional_results.values()
        ])
        
        print(f"   å¹³å‡æ”»æ“Šæ•ˆæœ: {avg_attack_effectiveness:.1%}")
        print(f"   å¹³å‡æ•¸æ“šä¿ç•™: {avg_data_preservation:.1%}")
    
    print("\nâœ… çµ„åˆæ”»æ“Šæ¼”ç¤ºå®Œæˆ!")


def example_5_nlp_pipeline_integration():
    """ç¤ºä¾‹5: NLPæµæ°´ç·šæ•´åˆ"""
    print("\n" + "="*60)
    print("ğŸ”— ç¤ºä¾‹5: NLPæµæ°´ç·šæ•´åˆ (NLP Pipeline Integration)")
    print("="*60)
    
    if not MAIN_MODULE_AVAILABLE:
        print("Main module not available, skipping example")
        return
    
    # æ¨¡æ“¬NLPæµæ°´ç·šæ•¸æ“š
    print("1. æ¨¡æ“¬NLPæµæ°´ç·šæ•¸æ“š...")
    
    # æ–‡æœ¬æ•¸æ“š
    sample_texts = [
        "Natural language processing is fascinating.",
        "Machine learning algorithms can reduce dimensionality effectively.",
        "The concept of dimensional reduction attack is inspired by science fiction.",
        "Flattening complex data structures improves computational efficiency.",
        "Text analysis benefits from dimensional reduction techniques."
    ]
    
    # æ¨¡æ“¬å¾æ–‡æœ¬æå–çš„é«˜ç¶­ç‰¹å¾µ (ä¾‹å¦‚BERT embeddings)
    text_features = generate_sample_high_dimensional_data(len(sample_texts), 768)  # BERTç¶­åº¦
    
    print(f"   æ–‡æœ¬æ•¸é‡: {len(sample_texts)}")
    print(f"   ç‰¹å¾µç¶­åº¦: {text_features.shape[1]} (æ¨¡æ“¬BERT)")
    
    # æ¨¡æ“¬è¤‡é›œçš„NLPè™•ç†çµæœ
    nlp_results = {
        'text_analysis': {
            'sentiment': {
                'positive': 0.8,
                'negative': 0.1,
                'neutral': 0.1
            },
            'entities': {
                'person': ['researchers', 'scientists'],
                'technology': ['NLP', 'machine learning', 'BERT'],
                'concepts': ['dimensionality', 'reduction', 'flattening']
            },
            'embeddings': {
                'sentence_embeddings': text_features.tolist(),
                'metadata': {
                    'model': 'BERT',
                    'dimensions': 768,
                    'processing_time': '2.5s'
                }
            }
        }
    }
    
    print("\n2. åŸ·è¡Œæ•´åˆé™ç¶­æ”»æ“Š...")
    
    orchestrator = DimensionalAttackOrchestrator()
    flattener = EarthFlattener()
    
    # å°æ–‡æœ¬ç‰¹å¾µåŸ·è¡Œé™ç¶­
    if 'pca' in orchestrator.list_available_attacks():
        # Adjust target dimensions based on available data
        max_dims = min(text_features.shape[0], text_features.shape[1]) - 1
        target_dims = min(50, max_dims)  # ç¢ºä¿ä¸è¶…éæœ€å¤§å¯ç”¨ç¶­åº¦
        
        feature_attack = orchestrator.execute_dimensional_attack(
            data=text_features,
            attack_method='pca',
            target_dimensions=target_dims  # å‹•æ…‹èª¿æ•´ç›®æ¨™ç¶­åº¦
        )
        
        print(f"   æ–‡æœ¬ç‰¹å¾µé™ç¶­: {feature_attack.original_dimensions}D â†’ {feature_attack.reduced_dimensions}D")
        print(f"   ä¿¡æ¯ä¿ç•™: {feature_attack.data_preserved_ratio:.1%}")
    
    # æ‰å¹³åŒ–NLPçµæœçµæ§‹
    structure_attack = flattener.flatten_nested_dict(nlp_results)
    
    print(f"   çµæ§‹æ‰å¹³åŒ–: {structure_attack.metadata['original_depth']} å±¤ â†’ 1 å±¤")
    print(f"   è¤‡é›œåº¦æ¸›å°‘: {structure_attack.complexity_reduction:.1%}")
    
    print("\n3. æ•´åˆå¾Œçš„æµæ°´ç·šæ•ˆç›Š:")
    print("   âœ“ é™ä½å­˜å„²éœ€æ±‚")
    print("   âœ“ æé«˜è¨ˆç®—æ•ˆç‡") 
    print("   âœ“ ç°¡åŒ–æ•¸æ“šè¨ªå•")
    print("   âœ“ ä¾¿æ–¼å¾ŒçºŒè™•ç†")
    
    print("\nâœ… NLPæµæ°´ç·šæ•´åˆç¤ºä¾‹å®Œæˆ!")


def main():
    """ä¸»å‡½æ•¸ - é‹è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸŒŸ é™ç¶­æ‰“æ“Šå’ŒFlattening Earth å¯¦ä¾‹æ¼”ç¤º")
    print("ğŸŒŸ Dimensional Reduction Attack and Flattening Earth Examples")
    print("="*80)
    
    if not MAIN_MODULE_AVAILABLE:
        print("âŒ éŒ¯èª¤: ç„¡æ³•å°å…¥ä¸»æ¨¡çµ„ï¼Œè«‹ç¢ºä¿ DimensionalReductionAttack.py åœ¨ç•¶å‰ç›®éŒ„")
        print("âŒ Error: Cannot import main module, ensure DimensionalReductionAttack.py is in current directory")
        return
    
    try:
        # é‹è¡Œæ‰€æœ‰ç¤ºä¾‹
        example_1_word_embedding_attack()
        example_2_document_feature_attack()
        example_3_earth_flattening()
        example_4_combined_attack()
        example_5_nlp_pipeline_integration()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹æ¼”ç¤ºå®Œæˆ! (All examples completed!)")
        print("ğŸ‰ é™ç¶­æ”»æ“Šå’Œåœ°çƒæ‰å¹³åŒ–åŠŸèƒ½å±•ç¤ºçµæŸ")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ é‹è¡Œç¤ºä¾‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥ä¾è³´é …æ˜¯å¦æ­£ç¢ºå®‰è£")


if __name__ == "__main__":
    main()